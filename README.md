# TEC-MoLLM: Total Electron Content Prediction using Multi-modal Large Language Model

A hybrid deep learning model for predicting Total Electron Content (TEC) in the ionosphere, combining Graph Neural Networks, Temporal Convolutional Networks, and Large Language Models with LoRA fine-tuning.

## ğŸŒŸ Features

- **Multi-modal Architecture**: Integrates spatial, temporal, and contextual features
- **Graph Neural Networks**: Captures geographical relationships using GATv2
- **Temporal Modeling**: Multi-scale convolutional embedding inspired by SeisMoLLM
- **LLM Integration**: GPT-2 backbone with LoRA for parameter-efficient fine-tuning
- **Comprehensive Evaluation**: Multiple metrics including MAE, RMSE, RÂ², and Pearson correlation

## ğŸ—ï¸ Architecture

```
Input Features (TEC + Space Weather) 
    â†“
Spatio-Temporal Embedding
    â†“
Spatial Encoder (GATv2)
    â†“
Temporal Encoder (Multi-scale Conv + Patching)
    â†“
LLM Backbone (GPT-2 + LoRA)
    â†“
Prediction Head
    â†“
TEC Predictions (12 horizons)
```

## ğŸ“Š Dataset

- **Data Source**: CRIM_SW2hr_AI ionosphere data (2014-2015)
- **Features**: 
  - TEC values (41Ã—71 spatial grid)
  - Space weather indices (5 parameters)
  - Temporal coordinates
- **Splits**: 
  - Training: 2014 (4,368 samples)
  - Validation: Jan-Jun 2015 (2,160 samples) 
  - Test: Jul-Dec 2015 (2,196 samples)

## ğŸš€ Quick Start

### Prerequisites

```bash
pip install torch torch-geometric transformers peft einops
pip install scikit-learn scipy h5py joblib tqdm
```

### Training

```bash
python train.py
```

### Project Structure

```
TEC-MoLLM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ data_loader.py      # HDF5 data loading
â”‚   â”‚   â””â”€â”€ dataset.py          # PyTorch sliding window dataset
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ feature_engineering.py  # Feature preprocessing
â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â””â”€â”€ graph_constructor.py     # Spatial graph construction
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ modules.py               # Core model components
â”‚   â”‚   â””â”€â”€ tec_mollm.py            # Main model assembly
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ metrics.py              # Evaluation metrics
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ baselines.py            # Baseline models (HA, SARIMA)
â”œâ”€â”€ tests/                          # Unit tests
â”œâ”€â”€ train.py                       # Training script
â”œâ”€â”€ test.py                        # Testing script
â””â”€â”€ data/
    â”œâ”€â”€ raw/                       # Original HDF5 files
    â””â”€â”€ processed/                 # Preprocessed data
```

## ğŸ“ˆ Results

### Current Performance (3 epochs, subset data)

| Metric | Value |
|--------|-------|
| MAE | 197.03 TECU |
| RMSE | 285.82 TECU |
| RÂ² | 0.668 |
| Pearson R | 0.820 |

### Training Progress

- **Epoch 1**: Train Loss: 468.45 â†’ Val Loss: 198.98
- **Epoch 2**: Train Loss: 221.73 â†’ Val Loss: 89.52
- **Epoch 3**: Continued improvement

## ğŸ”§ Model Components

### 1. Spatio-Temporal Embedding
- Node embeddings for 2,911 grid points
- Time-of-day and day-of-year embeddings
- Learnable 16-dimensional representations

### 2. Spatial Encoder (GATv2)
- Graph attention mechanism for spatial dependencies
- Edge connectivity based on geographical distance
- Multi-head attention (2 heads, 32 output channels)

### 3. Temporal Encoder
- Multi-scale convolutional blocks (kernel sizes: 3, 5, 7)
- Downsampling with strides [2, 2]: 24 â†’ 12 â†’ 6 timesteps
- Latent patching for LLM compatibility

### 4. LLM Backbone
- GPT-2 model truncated to 3 layers
- LoRA adaptation (r=16, Î±=32)
- Parameter-efficient fine-tuning (1.55% trainable parameters)

### 5. Prediction Head
- Linear projection to 12 prediction horizons
- Maps from LLM hidden dimension to TEC forecasts

## ğŸ¯ Future Improvements

### High Priority
- [ ] Use full dataset (4,368 training samples)
- [ ] Extend training to 50-100 epochs
- [ ] Implement real time features (hour/day_of_year)
- [ ] Increase input window size to 72-168 timesteps

### Medium Priority
- [ ] Advanced loss functions (Huber, MAE)
- [ ] Learning rate scheduling
- [ ] Data augmentation strategies
- [ ] Deeper LLM (6-12 layers)

### Low Priority
- [ ] Attention mechanisms for spatio-temporal fusion
- [ ] Physics-informed constraints
- [ ] Model ensemble methods
- [ ] Hyperparameter optimization

## ğŸ“š References

- **Graph Attention Networks**: VeliÄkoviÄ‡ et al. (2018)
- **LoRA**: Hu et al. (2021) - Low-Rank Adaptation of Large Language Models
- **SeisMoLLM**: Inspired by seismic modeling approaches
- **TEC Prediction**: Ionosphere research and space weather modeling

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ Contact

For questions and collaborations, please open an issue in this repository.

---

â­ **Star this repository if you find it helpful!**