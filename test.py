import torch
from torch.utils.data import DataLoader
import numpy as np
import logging
import os
import pandas as pd
from tqdm import tqdm
import argparse
import joblib
from sklearn.preprocessing import StandardScaler

from src.data.dataset import SlidingWindowSamplerDataset
from src.features.feature_engineering import create_features_and_targets, standardize_features
from src.model.tec_mollm import TEC_MoLLM
from src.evaluation.metrics import evaluate_horizons

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def get_tec_mollm_predictions(model, dataloader, device, edge_index):
    """è·å–TEC-MoLLMæ¨¡å‹çš„é¢„æµ‹ç»“æœ"""
    model.eval()
    all_preds = []
    all_trues = []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="TEC-MoLLM Inference"):
            x = batch['x'].to(device)
            y = batch['y'].to(device)
            time_features = batch['x_time_features'].to(device)
            
            # ä¸train.pyä¸­validateå‡½æ•°ç›¸åŒçš„reshapeé€»è¾‘
            B, L, H, W, C = x.shape
            x = x.view(B, L, H * W, C)
            # time_features shape should be (B, L, N, 2), so we need to expand the spatial dimension
            if time_features.numel() > 0:
                time_features = time_features.unsqueeze(-2).expand(B, L, H * W, -1)  # (B, L, N, 2)
            
            output = model(x, time_features, edge_index)
            # Reshape target to match output: (B, H, W, L_out) -> (B, L_out, H*W, 1)
            y_reshaped = y.permute(0, 3, 1, 2).reshape(B, -1, H * W, 1)
            
            all_preds.append(output.cpu().numpy())
            all_trues.append(y_reshaped.cpu().numpy())
    
    return np.concatenate(all_trues, axis=0), np.concatenate(all_preds, axis=0)

def get_baseline_predictions(test_data, L_in, L_out):
    """ç”Ÿæˆç®€å•çš„åŸºçº¿é¢„æµ‹"""
    logging.info("ç”Ÿæˆå†å²å¹³å‡åŸºçº¿é¢„æµ‹...")
    
    # è·å–æµ‹è¯•æ•°æ®å½¢çŠ¶
    B, H, W, features = test_data['X'].shape
    
    # ç®€å•çš„å†å²å¹³å‡ï¼šä½¿ç”¨è¾“å…¥åºåˆ—æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„å¹³å‡å€¼ä½œä¸ºæ‰€æœ‰æœªæ¥é¢„æµ‹
    # è¿™é‡Œæˆ‘ä»¬éœ€è¦åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬æ¥åŒ¹é…æ•°æ®æ ¼å¼
    predictions = []
    
    # éå†å¯èƒ½çš„æ»‘åŠ¨çª—å£
    for i in range(len(test_data['X']) - L_in - L_out + 1):
        # å–è¾“å…¥åºåˆ—çš„å†å²æ•°æ® (L_in, H, W, features)
        input_seq = test_data['X'][i:i+L_in]
        
        # è®¡ç®—å†å²å¹³å‡å€¼ (å¯¹æ—¶é—´ç»´åº¦æ±‚å¹³å‡)
        # åªä½¿ç”¨TECç‰¹å¾ï¼ˆç¬¬0ä¸ªç‰¹å¾ï¼‰è¿›è¡Œé¢„æµ‹
        historical_avg = np.mean(input_seq[:, :, :, 0:1], axis=0, keepdims=True)  # (1, H, W, 1)
        
        # é‡å¤L_outæ¬¡ä½œä¸ºæœªæ¥é¢„æµ‹
        pred = np.repeat(historical_avg, L_out, axis=0)  # (L_out, H, W, 1)
        predictions.append(pred)
    
    # è½¬æ¢ä¸ºä¸ç›®æ ‡ç›¸åŒçš„æ ¼å¼ (num_samples, H, W, L_out)
    predictions = np.array(predictions)  # (num_samples, L_out, H, W, 1)
    predictions = predictions.transpose(0, 2, 3, 1, 4).squeeze(-1)  # (num_samples, H, W, L_out)
    
    return predictions

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="è¯„ä¼°TEC-MoLLMå’ŒåŸºçº¿æ¨¡å‹")
    
    # æ•°æ®é…ç½®
    parser.add_argument('--L_in', type=int, default=48, help='è¾“å…¥åºåˆ—é•¿åº¦')
    parser.add_argument('--L_out', type=int, default=12, help='è¾“å‡ºåºåˆ—é•¿åº¦')
    parser.add_argument('--data_files', nargs='+', default=[
        'data/raw/CRIM_SW2hr_AI_v1.2_2014_DataDrivenRange_CN.hdf5',
        'data/raw/CRIM_SW2hr_AI_v1.2_2015_DataDrivenRange_CN.hdf5'
    ])
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--d_emb', type=int, default=16, help='åµŒå…¥ç»´åº¦')
    parser.add_argument('--llm_layers', type=int, default=3, help='LLMå±‚æ•°')
    
    # æ–‡ä»¶è·¯å¾„
    parser.add_argument('--scaler_path', type=str, default='data/processed/scaler.joblib')
    parser.add_argument('--target_scaler_path', type=str, default='data/processed/target_scaler.joblib')
    parser.add_argument('--graph_path', type=str, default='data/processed/graph_A.pt')
    parser.add_argument('--model_checkpoint', type=str, default='checkpoints/best_model.pth')
    parser.add_argument('--output_dir', type=str, default='results')
    parser.add_argument('--batch_size', type=int, default=16)
    
    return parser.parse_args()

def main():
    args = parse_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    logging.info(f"æµ‹è¯•é…ç½®: L_in={args.L_in}, L_out={args.L_out}")
    
    # --- åŠ è½½å’Œé¢„å¤„ç†æ•°æ® ---
    logging.info("åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    processed_data = create_features_and_targets(args.data_files)
    standardized_data, _ = standardize_features(processed_data, scaler_path=args.scaler_path)
    
    # åˆ›å»ºç›®æ ‡æ•°æ®çš„ç‹¬ç«‹ç¼©æ”¾å™¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists(args.target_scaler_path):
        logging.info("åˆ›å»ºç›®æ ‡æ•°æ®ç¼©æ”¾å™¨...")
        target_scaler = StandardScaler()
        train_tec_data = processed_data['train']['X'][:, :, :, 0:1]
        train_tec_reshaped = train_tec_data.reshape(-1, 1)
        target_scaler.fit(train_tec_reshaped)
        joblib.dump(target_scaler, args.target_scaler_path)
    
    # ä½¿ç”¨çœŸå®çš„æ—¶é—´ç‰¹å¾
    test_time_features = processed_data['test']['time_features']
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_dataset = SlidingWindowSamplerDataset(
        standardized_data['test']['X'], 
        standardized_data['test']['Y'], 
        test_time_features, 
        L_in=args.L_in, 
        L_out=args.L_out
    )
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    logging.info(f"æµ‹è¯•æ•°æ®é›†å¤§å°: {len(test_dataset)} æ ·æœ¬")
    
    # --- æ¨¡å‹é…ç½® ---
    # è®¡ç®—å·ç§¯åçš„æ—¶é—´åºåˆ—é•¿åº¦
    conv_output_len = args.L_in // (2 * 2)  # ä¸¤ä¸ªstride-2çš„å·ç§¯
    patch_len = 4
    num_patches = conv_output_len // patch_len
    
    if conv_output_len % patch_len != 0:
        patch_len = 2 if conv_output_len % 2 == 0 else 1
        num_patches = conv_output_len // patch_len
        logging.warning(f"è°ƒæ•´patch_lenä¸º{patch_len}ä»¥é€‚åº”conv_output_len {conv_output_len}")
    
    model_config = {
        "num_nodes": 2911, "d_emb": args.d_emb, "spatial_in_channels_base": 6,
        "spatial_out_channels": 32, "spatial_heads": 2, "temporal_channel_list": [64, 128],
        "temporal_strides": [2, 2], "patch_len": patch_len, "d_llm": 768, 
        "llm_layers": args.llm_layers, "prediction_horizon": args.L_out, 
        "temporal_seq_len": args.L_in
    }
    
    # --- åŠ è½½æ¨¡å‹å’Œè·å–é¢„æµ‹ ---
    results = {}
    
    # TEC-MoLLMé¢„æµ‹
    logging.info("åŠ è½½TEC-MoLLMæ¨¡å‹...")
    model = TEC_MoLLM(model_config).to(device)
    
    # åŠ è½½æ¨¡å‹æƒé‡ï¼ˆå¤„ç†DDPä¿å­˜çš„æ¨¡å‹ï¼‰
    checkpoint = torch.load(args.model_checkpoint, map_location=device)
    if isinstance(checkpoint, dict) and 'module.' in list(checkpoint.keys())[0]:
        # å¦‚æœæ˜¯DDPä¿å­˜çš„æ¨¡å‹ï¼Œå»é™¤'module.'å‰ç¼€
        new_checkpoint = {}
        for k, v in checkpoint.items():
            new_checkpoint[k.replace('module.', '')] = v
        checkpoint = new_checkpoint
    
    model.load_state_dict(checkpoint)
    edge_index = torch.load(args.graph_path)['edge_index'].to(device)
    
    logging.info("è·å–TEC-MoLLMé¢„æµ‹ç»“æœ...")
    y_true, y_pred_mollm = get_tec_mollm_predictions(model, test_loader, device, edge_index)
    
    # åŸºçº¿é¢„æµ‹
    logging.info("ç”ŸæˆåŸºçº¿é¢„æµ‹...")
    y_pred_ha = get_baseline_predictions(standardized_data['test'], args.L_in, args.L_out)
    
    # éœ€è¦å°†åŸºçº¿é¢„æµ‹é‡å¡‘ä¸ºä¸y_trueç›¸åŒçš„æ ¼å¼
    # y_true: (num_samples, L_out, H*W, 1)
    # y_pred_ha: (num_samples, H, W, L_out)
    B, H, W, L_out = y_pred_ha.shape
    y_pred_ha_reshaped = y_pred_ha.transpose(0, 3, 1, 2).reshape(B, L_out, H*W, 1)
    
    # ç¡®ä¿åŸºçº¿é¢„æµ‹çš„æ ·æœ¬æ•°ä¸çœŸå®å€¼åŒ¹é…
    min_samples = min(len(y_true), len(y_pred_ha_reshaped))
    y_true_matched = y_true[:min_samples]
    y_pred_ha_matched = y_pred_ha_reshaped[:min_samples]
    
    # --- è¯„ä¼°æŒ‡æ ‡ ---
    logging.info("è¯„ä¼°TEC-MoLLMæ¨¡å‹...")
    results['TEC-MoLLM'] = evaluate_horizons(y_true_matched, y_pred_mollm[:min_samples], args.target_scaler_path)
    
    logging.info("è¯„ä¼°å†å²å¹³å‡åŸºçº¿...")
    results['HistoricalAverage'] = evaluate_horizons(y_true_matched, y_pred_ha_matched, args.target_scaler_path)

    # --- æ ¼å¼åŒ–å’Œä¿å­˜ç»“æœ ---
    results_df = pd.DataFrame(results).T
    
    # æ·»åŠ è¯¦ç»†çš„ç»“æœå±•ç¤º
    logging.info("="*80)
    logging.info("æœ€ç»ˆè¯„ä¼°ç»“æœ")
    logging.info("="*80)
    
    for model_name, metrics in results.items():
        logging.info(f"\nğŸ“Š {model_name} æ¨¡å‹ç»“æœ:")
        logging.info(f"  å¹³å‡MAE:  {metrics['mae_avg']:.6f}")
        logging.info(f"  å¹³å‡RMSE: {metrics['rmse_avg']:.6f}")
        logging.info(f"  å¹³å‡RÂ²:   {metrics['r2_score_avg']:.6f}")
        logging.info(f"  å¹³å‡Pearson R: {metrics['pearson_r_avg']:.6f}")
        
        if 'mae_by_horizon' in metrics:
            logging.info("  å„é¢„æµ‹æ—¶æ­¥è¯¦ç»†æŒ‡æ ‡:")
            for h in range(len(metrics['mae_by_horizon'])):
                logging.info(f"    æ—¶æ­¥{h+1:2d}: MAE={metrics['mae_by_horizon'][h]:.6f}, "
                           f"RMSE={metrics['rmse_by_horizon'][h]:.6f}, "
                           f"RÂ²={metrics['r2_by_horizon'][h]:.6f}, "
                           f"Pearson R={metrics['pearson_by_horizon'][h]:.6f}")
    
    # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
    if 'TEC-MoLLM' in results and 'HistoricalAverage' in results:
        logging.info("\nğŸ¯ TEC-MoLLMç›¸å¯¹äºå†å²å¹³å‡çš„æ”¹è¿›:")
        tec_metrics = results['TEC-MoLLM']
        ha_metrics = results['HistoricalAverage']
        
        mae_improvement = ((ha_metrics['mae_avg'] - tec_metrics['mae_avg']) / ha_metrics['mae_avg']) * 100
        rmse_improvement = ((ha_metrics['rmse_avg'] - tec_metrics['rmse_avg']) / ha_metrics['rmse_avg']) * 100
        r2_improvement = ((tec_metrics['r2_score_avg'] - ha_metrics['r2_score_avg']) / abs(ha_metrics['r2_score_avg'])) * 100
        pearson_improvement = ((tec_metrics['pearson_r_avg'] - ha_metrics['pearson_r_avg']) / ha_metrics['pearson_r_avg']) * 100
        
        logging.info(f"  MAEæ”¹è¿›:     {mae_improvement:+.2f}%")
        logging.info(f"  RMSEæ”¹è¿›:    {rmse_improvement:+.2f}%") 
        logging.info(f"  RÂ²æ”¹è¿›:      {r2_improvement:+.2f}%")
        logging.info(f"  Pearson Ræ”¹è¿›: {pearson_improvement:+.2f}%")
    
    logging.info("="*80)
    
    # ä¿å­˜ç»“æœ
    output_path = os.path.join(args.output_dir, "evaluation_results.csv")
    results_df.to_csv(output_path)
    logging.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    # ä¿å­˜ç®€è¦ç»“æœæ‘˜è¦
    summary_path = os.path.join(args.output_dir, "evaluation_summary.txt")
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("TEC-MoLLMæ¨¡å‹è¯„ä¼°ç»“æœæ‘˜è¦\n")
        f.write("="*50 + "\n\n")
        
        for model_name, metrics in results.items():
            f.write(f"{model_name}:\n")
            f.write(f"  å¹³å‡MAE:  {metrics['mae_avg']:.6f}\n")
            f.write(f"  å¹³å‡RMSE: {metrics['rmse_avg']:.6f}\n")
            f.write(f"  å¹³å‡RÂ²:   {metrics['r2_score_avg']:.6f}\n")
            f.write(f"  å¹³å‡Pearson R: {metrics['pearson_r_avg']:.6f}\n\n")
    
    logging.info(f"ç»“æœæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")

if __name__ == '__main__':
    main() 