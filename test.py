import torch
from torch.utils.data import DataLoader
import numpy as np
import logging
import os
import pandas as pd
from tqdm import tqdm
import argparse
import joblib
from sklearn.preprocessing import StandardScaler

from src.data.dataset import SlidingWindowSamplerDataset
from src.model.tec_mollm import TEC_MoLLM
from src.evaluation.metrics import evaluate_horizons

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# --- ä¸€æ¬¡æ€§åŠ è½½scalerç¼“å­˜ ---
_cached_target_scaler = None
_cached_feature_scaler = None
_cached_tec_feature_scaler = None
_cached_scaler_paths = {}


def get_cached_scalers(target_scaler_path):
    """ç¼“å­˜scalerï¼Œé¿å…é‡å¤I/O"""
    global _cached_target_scaler, _cached_feature_scaler, _cached_tec_feature_scaler, _cached_scaler_paths

    if target_scaler_path not in _cached_scaler_paths:
        try:
            # åŠ è½½target_scaler
            _cached_target_scaler = joblib.load(target_scaler_path)

            # åŠ è½½feature_scaler
            feature_scaler_path = target_scaler_path.replace("target_scaler.joblib", "scaler.joblib")
            _cached_feature_scaler = joblib.load(feature_scaler_path)

            # åˆ›å»ºTECç‰¹å¾ä¸“ç”¨scaler
            from sklearn.preprocessing import StandardScaler

            _cached_tec_feature_scaler = StandardScaler()
            _cached_tec_feature_scaler.mean_ = np.array([_cached_feature_scaler.mean_[0]])
            _cached_tec_feature_scaler.scale_ = np.array([_cached_feature_scaler.scale_[0]])

            _cached_scaler_paths[target_scaler_path] = True
            logging.info(f"Cached scalers loaded from {target_scaler_path}")

        except Exception as e:
            logging.error(f"Failed to load scalers: {e}")
            return None, None, None

    return _cached_target_scaler, _cached_feature_scaler, _cached_tec_feature_scaler


def inverse_transform_output(tensor_scaled: torch.Tensor, scaler) -> torch.Tensor:
    """
    è¾…åŠ©å‡½æ•°ï¼šå°†æ ‡å‡†åŒ–çš„å¼ é‡é€šè¿‡scalerè¿›è¡Œé€†å˜æ¢

    Args:
        tensor_scaled (torch.Tensor): æ ‡å‡†åŒ–çš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, ..., C) æˆ– (B, ...)
        scaler: å·²æ‹Ÿåˆçš„StandardScaler

    Returns:
        torch.Tensor: é€†å˜æ¢åçš„å¼ é‡ï¼Œä¿æŒåŸå§‹å½¢çŠ¶
    """
    original_shape = tensor_scaled.shape

    # å¦‚æœæ˜¯å•ç‰¹å¾å¼ é‡ï¼Œéœ€è¦æ·»åŠ ç‰¹å¾ç»´åº¦
    if len(original_shape) == 3:  # (B, H, W) -> (B, H, W, 1)
        tensor_reshaped = tensor_scaled.reshape(-1, 1)
    else:  # (B, H, W, C) -> (B*H*W*C, 1) for single feature
        tensor_reshaped = tensor_scaled.reshape(-1, 1)

    # è½¬æ¢ä¸ºnumpyè¿›è¡Œé€†å˜æ¢
    tensor_numpy = tensor_reshaped.cpu().numpy()
    tensor_unscaled_numpy = scaler.inverse_transform(tensor_numpy)

    # è½¬æ¢å›tensorå¹¶æ¢å¤åŸå§‹å½¢çŠ¶
    tensor_unscaled = torch.from_numpy(tensor_unscaled_numpy).to(tensor_scaled.device)
    tensor_unscaled = tensor_unscaled.reshape(original_shape)

    return tensor_unscaled


def transform_output(tensor_unscaled: torch.Tensor, scaler) -> torch.Tensor:
    """
    è¾…åŠ©å‡½æ•°ï¼šå°†ç‰©ç†å€¼å¼ é‡é€šè¿‡scalerè¿›è¡Œæ ‡å‡†åŒ–

    Args:
        tensor_unscaled (torch.Tensor): ç‰©ç†å€¼å¼ é‡
        scaler: å·²æ‹Ÿåˆçš„StandardScaler

    Returns:
        torch.Tensor: æ ‡å‡†åŒ–åçš„å¼ é‡ï¼Œä¿æŒåŸå§‹å½¢çŠ¶
    """
    original_shape = tensor_unscaled.shape

    # é‡å¡‘ä¸ºscaleræœŸæœ›çš„å½¢çŠ¶
    if len(original_shape) == 3:  # (B, H, W) -> (B, H, W, 1)
        tensor_reshaped = tensor_unscaled.reshape(-1, 1)
    else:  # (B, H, W, C) -> (B*H*W*C, 1) for single feature
        tensor_reshaped = tensor_unscaled.reshape(-1, 1)

    # è½¬æ¢ä¸ºnumpyè¿›è¡Œæ ‡å‡†åŒ–
    tensor_numpy = tensor_reshaped.cpu().numpy()
    tensor_scaled_numpy = scaler.transform(tensor_numpy)

    # è½¬æ¢å›tensorå¹¶æ¢å¤åŸå§‹å½¢çŠ¶
    tensor_scaled = torch.from_numpy(tensor_scaled_numpy).to(tensor_unscaled.device)
    tensor_scaled = tensor_scaled.reshape(original_shape)

    return tensor_scaled


def get_tec_mollm_predictions(model, dataloader, device, edge_index, target_scaler_path):
    """è·å–TEC-MoLLMæ¨¡å‹çš„é¢„æµ‹ç»“æœ"""
    model.eval()
    all_preds = []
    all_trues = []

    # --- ä½¿ç”¨ç¼“å­˜çš„scalerï¼Œé¿å…é‡å¤I/O ---
    target_scaler, feature_scaler, tec_feature_scaler = get_cached_scalers(target_scaler_path)
    if target_scaler is None:
        logging.error("Failed to load scalers")
        return np.array([]), np.array([])
    # --- END MODIFICATION ---

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="TEC-MoLLM Inference"):
            x = batch["x"].to(device)
            y_residual_scaled = batch["y"].to(device)
            time_features = batch["x_time_features"].to(device)

            # ä¸train.pyä¸­validateå‡½æ•°ç›¸åŒçš„reshapeé€»è¾‘
            B, L, H, W, C = x.shape
            x = x.view(B, L, H * W, C)
            # time_features shape should be (B, L, N, 2), so we need to expand the spatial dimension
            if time_features.numel() > 0:
                time_features = time_features.unsqueeze(-2).expand(B, L, H * W, -1)  # (B, L, N, 2)

            # 1. Get model's residual prediction (scaled)
            output_residual_scaled = model(x, time_features, edge_index)
            # Reshape target to match output: (B, H, W, L_out) -> (B, L_out, H*W, 1)
            y_reshaped = y_residual_scaled.permute(0, 3, 1, 2).reshape(B, -1, H * W, 1)

            # --- START MODIFICATION: Reconstruct to absolute values and re-scale ---
            # 2. Get persistence baseline (this is the last known TEC value from input X)
            # It's the first channel of the last time step. Shape: (B, H*W, 1)
            persistence_baseline_from_x_scaled = x[:, -1, :, 0].unsqueeze(-1)  # (B, H*W, 1)

            # 3. Inverse transform all parts to get physical values
            # Note: output is (B, L_out, H*W, 1), need to handle properly
            output_residual_unscaled = inverse_transform_output(output_residual_scaled, target_scaler)
            y_residual_unscaled = inverse_transform_output(y_reshaped, target_scaler)

            # Baseline is from X, so use the correct TEC feature scaler
            persistence_baseline_unscaled = inverse_transform_output(persistence_baseline_from_x_scaled, tec_feature_scaler)

            # 4. Reconstruct absolute physical values
            # Expand baseline to match the prediction horizon
            L_out = output_residual_unscaled.shape[1]
            persistence_baseline_expanded = persistence_baseline_unscaled.unsqueeze(1).expand(-1, L_out, -1, -1)

            output_absolute_unscaled = persistence_baseline_expanded + output_residual_unscaled
            target_absolute_unscaled = persistence_baseline_expanded + y_residual_unscaled

            # 5. Re-scale the absolute values to feed into metrics.py
            # Use tec_feature_scaler for absolute values to maintain consistency
            output_final_scaled = transform_output(output_absolute_unscaled, tec_feature_scaler)
            target_final_scaled = transform_output(target_absolute_unscaled, tec_feature_scaler)

            all_preds.append(output_final_scaled.cpu().numpy())
            all_trues.append(target_final_scaled.cpu().numpy())
            # --- END MODIFICATION ---

    return np.concatenate(all_trues, axis=0), np.concatenate(all_preds, axis=0)


def get_baseline_predictions(test_dataset, L_in, L_out):
    """ç”Ÿæˆç®€å•çš„åŸºçº¿é¢„æµ‹ï¼ˆæ®‹å·®å½¢å¼ï¼‰"""
    logging.info("ç”ŸæˆæŒä¹…åŒ–æ¨¡å‹åŸºçº¿é¢„æµ‹ï¼ˆæ®‹å·®å½¢å¼ï¼‰...")

    predictions = []

    # éå†æµ‹è¯•æ•°æ®é›†æ ·æœ¬
    for i in range(len(test_dataset)):
        sample = test_dataset[i]
        x_window = sample["x"].numpy()  # (L_in, H, W, C)

        # --- START MODIFICATION: Generate residual baseline predictions ---
        # REASON: Since we're now predicting residuals, the persistence model baseline
        #         should predict zero residuals (future = current, so residual = 0)

        # For persistence model in residual space, the prediction is always 0
        # because persistence assumes future values equal current values
        # So: residual = future - current = current - current = 0

        H, W = x_window.shape[1], x_window.shape[2]
        pred_residual = np.zeros((L_out, H, W, 1))  # All zeros for residual prediction

        # è½¬æ¢ä¸ºæœŸæœ›çš„æ ¼å¼ (H, W, L_out)
        pred_reshaped = pred_residual.transpose(1, 2, 0, 3).squeeze(-1)  # (H, W, L_out)
        predictions.append(pred_reshaped)
        # --- END MODIFICATION ---

    # è½¬æ¢ä¸ºæœ€ç»ˆæ ¼å¼ (num_samples, H, W, L_out)
    predictions = np.array(predictions)

    return predictions


def find_latest_checkpoint(checkpoint_dir="checkpoints"):
    """æŸ¥æ‰¾æœ€æ–°çš„checkpointæ–‡ä»¶"""
    import glob

    pattern = os.path.join(checkpoint_dir, "best_model_*.pth")
    checkpoint_files = glob.glob(pattern)

    if not checkpoint_files:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŠ¨æ€å‘½åçš„æ–‡ä»¶ï¼Œå°è¯•é»˜è®¤æ–‡ä»¶
        default_path = os.path.join(checkpoint_dir, "best_model.pth")
        if os.path.exists(default_path):
            return default_path
        else:
            raise FileNotFoundError(f"No checkpoint files found in {checkpoint_dir}")

    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
    latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)
    return latest_checkpoint


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="è¯„ä¼°TEC-MoLLMå’ŒåŸºçº¿æ¨¡å‹")

    # æ•°æ®é…ç½®
    parser.add_argument("--L_in", type=int, default=48, help="è¾“å…¥åºåˆ—é•¿åº¦")
    parser.add_argument("--L_out", type=int, default=12, help="è¾“å‡ºåºåˆ—é•¿åº¦")

    # æ¨¡å‹é…ç½®
    parser.add_argument("--d_emb", type=int, default=16, help="åµŒå…¥ç»´åº¦")
    parser.add_argument("--llm_layers", type=int, default=3, help="LLMå±‚æ•°")

    # æ–‡ä»¶è·¯å¾„
    parser.add_argument("--target_scaler_path", type=str, default="data/processed/target_scaler.joblib")
    parser.add_argument("--graph_path", type=str, default="data/processed/graph_A.pt")
    parser.add_argument(
        "--model_checkpoint",
        type=str,
        default="checkpoints/best_model.pth",
        help='Path to model checkpoint. Use "latest" to auto-find the most recent checkpoint.',
    )
    parser.add_argument("--output_dir", type=str, default="results")
    parser.add_argument("--batch_size", type=int, default=16)

    return parser.parse_args()


def main():
    args = parse_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    logging.info(f"æµ‹è¯•é…ç½®: L_in={args.L_in}, L_out={args.L_out}")

    # --- éªŒè¯é¢„å¤„ç†æ•°æ®æ–‡ä»¶å­˜åœ¨ ---
    data_dir = "data/processed"
    if not os.path.exists(args.target_scaler_path):
        logging.error(f"Target scaler not found at {args.target_scaler_path}. Please run preprocessing script first.")
        return

    logging.info("åŠ è½½é¢„å¤„ç†å¥½çš„æµ‹è¯•æ•°æ®...")

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®ï¼‰
    test_dataset = SlidingWindowSamplerDataset(data_path=data_dir, mode="test", L_in=args.L_in, L_out=args.L_out)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)

    logging.info(f"æµ‹è¯•æ•°æ®é›†å¤§å°: {len(test_dataset)} æ ·æœ¬")

    # --- æ¨¡å‹é…ç½® ---
    # è®¡ç®—å·ç§¯åçš„æ—¶é—´åºåˆ—é•¿åº¦
    conv_output_len = args.L_in // (2 * 2)  # ä¸¤ä¸ªstride-2çš„å·ç§¯
    patch_len = 4
    num_patches = conv_output_len // patch_len

    if conv_output_len % patch_len != 0:
        patch_len = 2 if conv_output_len % 2 == 0 else 1
        num_patches = conv_output_len // patch_len
        logging.warning(f"è°ƒæ•´patch_lenä¸º{patch_len}ä»¥é€‚åº”conv_output_len {conv_output_len}")

    model_config = {
        "num_nodes": 2911,
        "d_emb": args.d_emb,
        "spatial_in_channels_base": 6,
        "spatial_out_channels": 11,
        "spatial_heads": 2,
        "temporal_channel_list": [64, 128],  # ä¿®æ”¹ä¸º11*2=22ï¼Œä¸è¾“å…¥ç»´åº¦ä¸€è‡´ç”¨äºæ®‹å·®è¿æ¥
        "temporal_strides": [2, 2],
        "patch_len": patch_len,
        "d_llm": 768,
        "llm_layers": args.llm_layers,
        "prediction_horizon": args.L_out,
        "temporal_seq_len": args.L_in,
        "num_years": 13,
    }

    # --- åŠ è½½æ¨¡å‹å’Œè·å–é¢„æµ‹ ---
    results = {}

    # TEC-MoLLMé¢„æµ‹
    logging.info("åŠ è½½TEC-MoLLMæ¨¡å‹...")
    model = TEC_MoLLM(model_config).to(device)

    # --- START MODIFICATION: Support dynamic checkpoint naming ---
    # å¤„ç†"latest"é€‰é¡¹æˆ–ç›´æ¥ä½¿ç”¨æŒ‡å®šçš„checkpointè·¯å¾„
    if args.model_checkpoint == "latest":
        checkpoint_path = find_latest_checkpoint()
        logging.info(f"è‡ªåŠ¨æ‰¾åˆ°æœ€æ–°checkpoint: {checkpoint_path}")
    else:
        checkpoint_path = args.model_checkpoint
        logging.info(f"ä½¿ç”¨æŒ‡å®šcheckpoint: {checkpoint_path}")

    # åŠ è½½æ¨¡å‹æƒé‡ï¼ˆå¤„ç†DDPå’Œtorch.compileä¿å­˜çš„æ¨¡å‹ï¼‰
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    # --- END MODIFICATION ---

    # åˆ›å»ºä¸€ä¸ªæ–°çš„state_dictæ¥å­˜å‚¨ä¿®å¤åçš„é”®
    new_state_dict = {}
    for k, v in checkpoint.items():
        # ç§»é™¤ 'module.' å‰ç¼€ (æ¥è‡ªDDP)
        if k.startswith("module."):
            k = k[len("module.") :]
        # ç§»é™¤ '_orig_mod.' å‰ç¼€ (æ¥è‡ªtorch.compile)
        if k.startswith("_orig_mod."):
            k = k[len("_orig_mod.") :]
        new_state_dict[k] = v

    # ä½¿ç”¨ä¿®å¤åçš„state_dictåŠ è½½æ¨¡å‹
    model.load_state_dict(new_state_dict)

    edge_index = torch.load(args.graph_path, weights_only=False)["edge_index"].to(device)

    logging.info("è·å–TEC-MoLLMé¢„æµ‹ç»“æœ...")
    y_true, y_pred_mollm = get_tec_mollm_predictions(model, test_loader, device, edge_index, args.target_scaler_path)

    # ä½¿ç”¨å·²ç¼“å­˜çš„scalerï¼ˆåœ¨get_tec_mollm_predictionsä¸­å·²åŠ è½½ï¼‰
    target_scaler, feature_scaler, tec_feature_scaler = get_cached_scalers(args.target_scaler_path)

    # åŸºçº¿é¢„æµ‹
    logging.info("ç”ŸæˆåŸºçº¿é¢„æµ‹...")
    y_pred_ha = get_baseline_predictions(test_dataset, args.L_in, args.L_out)

    # --- START MODIFICATION: Process baseline predictions consistently ---
    # REASON: Need to ensure baseline predictions are in the same scale as TEC-MoLLM predictions
    #         for fair comparison. Since TEC-MoLLM predictions are already processed to absolute
    #         values and re-scaled, we need to do the same for baseline.

    # Need to align dimensions: y_pred_ha is currently residuals (zeros)
    # We need to convert it to absolute values by adding persistence baseline
    logging.info("å¤„ç†åŸºçº¿é¢„æµ‹ä»¥ä¿æŒä¸€è‡´æ€§...")

    # ä½¿ç”¨å·²ç¼“å­˜çš„scaler
    target_scaler, _, _ = get_cached_scalers(args.target_scaler_path)

    baseline_absolute_predictions = []
    for i in range(len(test_dataset)):
        sample = test_dataset[i]
        x_window = sample["x"].numpy()  # (L_in, H, W, C)

        # Get the last known TEC value (persistence baseline)
        last_tec_value = x_window[-1, :, :, 0]  # (H, W)

        # Create persistence prediction (repeat last value for L_out steps)
        persistence_pred = np.tile(last_tec_value[:, :, np.newaxis], (1, 1, args.L_out))  # (H, W, L_out)

        baseline_absolute_predictions.append(persistence_pred)

    y_pred_ha_absolute = np.array(baseline_absolute_predictions)  # (num_samples, H, W, L_out)

    # Convert to same format as TEC-MoLLM predictions: (num_samples, L_out, H*W, 1)
    B, H, W, L_out = y_pred_ha_absolute.shape
    y_pred_ha_reshaped = y_pred_ha_absolute.transpose(0, 3, 1, 2).reshape(B, L_out, H * W, 1)

    # Apply TEC feature scaler normalization to match TEC-MoLLM prediction format
    y_pred_ha_scaled = transform_output(torch.from_numpy(y_pred_ha_reshaped).float(), tec_feature_scaler).numpy()
    # --- END MODIFICATION ---

    # ç¡®ä¿åŸºçº¿é¢„æµ‹çš„æ ·æœ¬æ•°ä¸çœŸå®å€¼åŒ¹é…
    min_samples = min(len(y_true), len(y_pred_ha_scaled))
    y_true_matched = y_true[:min_samples]
    y_pred_ha_matched = y_pred_ha_scaled[:min_samples]

    # --- è¯„ä¼°æŒ‡æ ‡ ---
    # Save temporary TEC scaler for evaluation
    import tempfile

    with tempfile.NamedTemporaryFile(delete=False, suffix="_tec_scaler.joblib") as temp_file:
        temp_scaler_path = temp_file.name
        joblib.dump(tec_feature_scaler, temp_scaler_path)

    logging.info("è¯„ä¼°TEC-MoLLMæ¨¡å‹...")
    results["TEC-MoLLM"] = evaluate_horizons(y_true_matched, y_pred_mollm[:min_samples], temp_scaler_path)

    logging.info("è¯„ä¼°å†å²å¹³å‡åŸºçº¿...")
    results["HistoricalAverage"] = evaluate_horizons(y_true_matched, y_pred_ha_matched, temp_scaler_path)

    # Clean up temporary file
    os.unlink(temp_scaler_path)

    # --- æ ¼å¼åŒ–å’Œä¿å­˜ç»“æœ ---
    results_df = pd.DataFrame(results).T

    # æ·»åŠ è¯¦ç»†çš„ç»“æœå±•ç¤º
    logging.info("=" * 80)
    logging.info("æœ€ç»ˆè¯„ä¼°ç»“æœ")
    logging.info("=" * 80)

    for model_name, metrics in results.items():
        logging.info(f"\nğŸ“Š {model_name} æ¨¡å‹ç»“æœ:")
        logging.info(f"  å¹³å‡MAE:  {metrics['mae_avg']:.6f}")
        logging.info(f"  å¹³å‡RMSE: {metrics['rmse_avg']:.6f}")
        logging.info(f"  å¹³å‡RÂ²:   {metrics['r2_score_avg']:.6f}")
        logging.info(f"  å¹³å‡Pearson R: {metrics['pearson_r_avg']:.6f}")

        if "mae_by_horizon" in metrics:
            logging.info("  å„é¢„æµ‹æ—¶æ­¥è¯¦ç»†æŒ‡æ ‡:")
            for h in range(len(metrics["mae_by_horizon"])):
                logging.info(
                    f"    æ—¶æ­¥{h+1:2d}: MAE={metrics['mae_by_horizon'][h]:.6f}, "
                    f"RMSE={metrics['rmse_by_horizon'][h]:.6f}, "
                    f"RÂ²={metrics['r2_by_horizon'][h]:.6f}, "
                    f"Pearson R={metrics['pearson_by_horizon'][h]:.6f}"
                )

    # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
    if "TEC-MoLLM" in results and "HistoricalAverage" in results:
        logging.info("\nğŸ¯ TEC-MoLLMç›¸å¯¹äºå†å²å¹³å‡çš„æ”¹è¿›:")
        tec_metrics = results["TEC-MoLLM"]
        ha_metrics = results["HistoricalAverage"]

        mae_improvement = ((ha_metrics["mae_avg"] - tec_metrics["mae_avg"]) / ha_metrics["mae_avg"]) * 100
        rmse_improvement = ((ha_metrics["rmse_avg"] - tec_metrics["rmse_avg"]) / ha_metrics["rmse_avg"]) * 100
        r2_improvement = ((tec_metrics["r2_score_avg"] - ha_metrics["r2_score_avg"]) / abs(ha_metrics["r2_score_avg"])) * 100
        pearson_improvement = ((tec_metrics["pearson_r_avg"] - ha_metrics["pearson_r_avg"]) / ha_metrics["pearson_r_avg"]) * 100

        logging.info(f"  MAEæ”¹è¿›:     {mae_improvement:+.2f}%")
        logging.info(f"  RMSEæ”¹è¿›:    {rmse_improvement:+.2f}%")
        logging.info(f"  RÂ²æ”¹è¿›:      {r2_improvement:+.2f}%")
        logging.info(f"  Pearson Ræ”¹è¿›: {pearson_improvement:+.2f}%")

    logging.info("=" * 80)

    # ä¿å­˜ç»“æœ
    output_path = os.path.join(args.output_dir, "evaluation_results.csv")
    results_df.to_csv(output_path)
    logging.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

    # ä¿å­˜ç®€è¦ç»“æœæ‘˜è¦
    summary_path = os.path.join(args.output_dir, "evaluation_summary.txt")
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write("TEC-MoLLMæ¨¡å‹è¯„ä¼°ç»“æœæ‘˜è¦\n")
        f.write("=" * 50 + "\n\n")

        for model_name, metrics in results.items():
            f.write(f"{model_name}:\n")
            f.write(f"  å¹³å‡MAE:  {metrics['mae_avg']:.6f}\n")
            f.write(f"  å¹³å‡RMSE: {metrics['rmse_avg']:.6f}\n")
            f.write(f"  å¹³å‡RÂ²:   {metrics['r2_score_avg']:.6f}\n")
            f.write(f"  å¹³å‡Pearson R: {metrics['pearson_r_avg']:.6f}\n\n")

    logging.info(f"ç»“æœæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")


if __name__ == "__main__":
    main()
