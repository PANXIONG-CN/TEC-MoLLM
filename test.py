import torch
from torch.utils.data import DataLoader
import numpy as np
import logging
import os
import pandas as pd
from tqdm import tqdm
import argparse
import joblib
from sklearn.preprocessing import StandardScaler

from src.data.dataset import SlidingWindowSamplerDataset
from src.model.tec_mollm import TEC_MoLLM
from src.evaluation.metrics import evaluate_horizons

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def get_tec_mollm_predictions(model, dataloader, device, edge_index):
    """è·å–TEC-MoLLMæ¨¡å‹çš„é¢„æµ‹ç»“æœ"""
    model.eval()
    all_preds = []
    all_trues = []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="TEC-MoLLM Inference"):
            x = batch['x'].to(device)
            y = batch['y'].to(device)
            time_features = batch['x_time_features'].to(device)
            
            # ä¸train.pyä¸­validateå‡½æ•°ç›¸åŒçš„reshapeé€»è¾‘
            B, L, H, W, C = x.shape
            x = x.view(B, L, H * W, C)
            # time_features shape should be (B, L, N, 2), so we need to expand the spatial dimension
            if time_features.numel() > 0:
                time_features = time_features.unsqueeze(-2).expand(B, L, H * W, -1)  # (B, L, N, 2)
            
            output = model(x, time_features, edge_index)
            # Reshape target to match output: (B, H, W, L_out) -> (B, L_out, H*W, 1)
            y_reshaped = y.permute(0, 3, 1, 2).reshape(B, -1, H * W, 1)
            
            all_preds.append(output.cpu().numpy())
            all_trues.append(y_reshaped.cpu().numpy())
    
    return np.concatenate(all_trues, axis=0), np.concatenate(all_preds, axis=0)

def get_baseline_predictions(test_dataset, L_in, L_out):
    """ç”Ÿæˆç®€å•çš„åŸºçº¿é¢„æµ‹"""
    logging.info("ç”Ÿæˆå†å²å¹³å‡åŸºçº¿é¢„æµ‹...")
    
    predictions = []
    
    # éå†æµ‹è¯•æ•°æ®é›†æ ·æœ¬
    for i in range(len(test_dataset)):
        sample = test_dataset[i]
        x_window = sample['x'].numpy()  # (L_in, H, W, C)
        
        # è®¡ç®—å†å²å¹³å‡å€¼ (å¯¹æ—¶é—´ç»´åº¦æ±‚å¹³å‡)
        # åªä½¿ç”¨TECç‰¹å¾ï¼ˆç¬¬0ä¸ªç‰¹å¾ï¼‰è¿›è¡Œé¢„æµ‹
        historical_avg = np.mean(x_window[:, :, :, 0:1], axis=0, keepdims=True)  # (1, H, W, 1)
        
        # é‡å¤L_outæ¬¡ä½œä¸ºæœªæ¥é¢„æµ‹
        pred = np.repeat(historical_avg, L_out, axis=0)  # (L_out, H, W, 1)
        
        # è½¬æ¢ä¸ºæœŸæœ›çš„æ ¼å¼ (H, W, L_out)
        pred_reshaped = pred.transpose(1, 2, 0, 3).squeeze(-1)  # (H, W, L_out)
        predictions.append(pred_reshaped)
    
    # è½¬æ¢ä¸ºæœ€ç»ˆæ ¼å¼ (num_samples, H, W, L_out)
    predictions = np.array(predictions)
    
    return predictions

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="è¯„ä¼°TEC-MoLLMå’ŒåŸºçº¿æ¨¡å‹")
    
    # æ•°æ®é…ç½®
    parser.add_argument('--L_in', type=int, default=48, help='è¾“å…¥åºåˆ—é•¿åº¦')
    parser.add_argument('--L_out', type=int, default=12, help='è¾“å‡ºåºåˆ—é•¿åº¦')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--d_emb', type=int, default=16, help='åµŒå…¥ç»´åº¦')
    parser.add_argument('--llm_layers', type=int, default=3, help='LLMå±‚æ•°')
    
    # æ–‡ä»¶è·¯å¾„
    parser.add_argument('--target_scaler_path', type=str, default='data/processed/target_scaler.joblib')
    parser.add_argument('--graph_path', type=str, default='data/processed/graph_A.pt')
    parser.add_argument('--model_checkpoint', type=str, default='checkpoints/best_model.pth')
    parser.add_argument('--output_dir', type=str, default='results')
    parser.add_argument('--batch_size', type=int, default=16)
    
    return parser.parse_args()

def main():
    args = parse_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    logging.info(f"æµ‹è¯•é…ç½®: L_in={args.L_in}, L_out={args.L_out}")
    
    # --- éªŒè¯é¢„å¤„ç†æ•°æ®æ–‡ä»¶å­˜åœ¨ ---
    data_dir = 'data/processed'
    if not os.path.exists(args.target_scaler_path):
        logging.error(f"Target scaler not found at {args.target_scaler_path}. Please run preprocessing script first.")
        return
    
    logging.info("åŠ è½½é¢„å¤„ç†å¥½çš„æµ‹è¯•æ•°æ®...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®ï¼‰
    test_dataset = SlidingWindowSamplerDataset(
        data_path=data_dir, 
        mode='test',
        L_in=args.L_in, 
        L_out=args.L_out
    )
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    logging.info(f"æµ‹è¯•æ•°æ®é›†å¤§å°: {len(test_dataset)} æ ·æœ¬")
    
    # --- æ¨¡å‹é…ç½® ---
    # è®¡ç®—å·ç§¯åçš„æ—¶é—´åºåˆ—é•¿åº¦
    conv_output_len = args.L_in // (2 * 2)  # ä¸¤ä¸ªstride-2çš„å·ç§¯
    patch_len = 4
    num_patches = conv_output_len // patch_len
    
    if conv_output_len % patch_len != 0:
        patch_len = 2 if conv_output_len % 2 == 0 else 1
        num_patches = conv_output_len // patch_len
        logging.warning(f"è°ƒæ•´patch_lenä¸º{patch_len}ä»¥é€‚åº”conv_output_len {conv_output_len}")
    
    model_config = {
        "num_nodes": 2911, "d_emb": args.d_emb, "spatial_in_channels_base": 6,
        "spatial_out_channels": 32, "spatial_heads": 2, "temporal_channel_list": [64, 128],
        "temporal_strides": [2, 2], "patch_len": patch_len, "d_llm": 768, 
        "llm_layers": args.llm_layers, "prediction_horizon": args.L_out, 
        "temporal_seq_len": args.L_in
    }
    
    # --- åŠ è½½æ¨¡å‹å’Œè·å–é¢„æµ‹ ---
    results = {}
    
    # TEC-MoLLMé¢„æµ‹
    logging.info("åŠ è½½TEC-MoLLMæ¨¡å‹...")
    model = TEC_MoLLM(model_config).to(device)
    
    # åŠ è½½æ¨¡å‹æƒé‡ï¼ˆå¤„ç†DDPå’Œtorch.compileä¿å­˜çš„æ¨¡å‹ï¼‰
    checkpoint = torch.load(args.model_checkpoint, map_location=device)
    
    # åˆ›å»ºä¸€ä¸ªæ–°çš„state_dictæ¥å­˜å‚¨ä¿®å¤åçš„é”®
    new_state_dict = {}
    for k, v in checkpoint.items():
        # ç§»é™¤ 'module.' å‰ç¼€ (æ¥è‡ªDDP)
        if k.startswith('module.'):
            k = k[len('module.'):]
        # ç§»é™¤ '_orig_mod.' å‰ç¼€ (æ¥è‡ªtorch.compile)
        if k.startswith('_orig_mod.'):
            k = k[len('_orig_mod.'):]
        new_state_dict[k] = v
    
    # ä½¿ç”¨ä¿®å¤åçš„state_dictåŠ è½½æ¨¡å‹
    model.load_state_dict(new_state_dict)
    
    edge_index = torch.load(args.graph_path)['edge_index'].to(device)
    
    logging.info("è·å–TEC-MoLLMé¢„æµ‹ç»“æœ...")
    y_true, y_pred_mollm = get_tec_mollm_predictions(model, test_loader, device, edge_index)
    
    # åŸºçº¿é¢„æµ‹
    logging.info("ç”ŸæˆåŸºçº¿é¢„æµ‹...")
    y_pred_ha = get_baseline_predictions(test_dataset, args.L_in, args.L_out)
    
    # éœ€è¦å°†åŸºçº¿é¢„æµ‹é‡å¡‘ä¸ºä¸y_trueç›¸åŒçš„æ ¼å¼
    # y_true: (num_samples, L_out, H*W, 1)
    # y_pred_ha: (num_samples, H, W, L_out)
    B, H, W, L_out = y_pred_ha.shape
    y_pred_ha_reshaped = y_pred_ha.transpose(0, 3, 1, 2).reshape(B, L_out, H*W, 1)
    
    # ç¡®ä¿åŸºçº¿é¢„æµ‹çš„æ ·æœ¬æ•°ä¸çœŸå®å€¼åŒ¹é…
    min_samples = min(len(y_true), len(y_pred_ha_reshaped))
    y_true_matched = y_true[:min_samples]
    y_pred_ha_matched = y_pred_ha_reshaped[:min_samples]
    
    # --- è¯„ä¼°æŒ‡æ ‡ ---
    logging.info("è¯„ä¼°TEC-MoLLMæ¨¡å‹...")
    results['TEC-MoLLM'] = evaluate_horizons(y_true_matched, y_pred_mollm[:min_samples], args.target_scaler_path)
    
    logging.info("è¯„ä¼°å†å²å¹³å‡åŸºçº¿...")
    results['HistoricalAverage'] = evaluate_horizons(y_true_matched, y_pred_ha_matched, args.target_scaler_path)

    # --- æ ¼å¼åŒ–å’Œä¿å­˜ç»“æœ ---
    results_df = pd.DataFrame(results).T
    
    # æ·»åŠ è¯¦ç»†çš„ç»“æœå±•ç¤º
    logging.info("="*80)
    logging.info("æœ€ç»ˆè¯„ä¼°ç»“æœ")
    logging.info("="*80)
    
    for model_name, metrics in results.items():
        logging.info(f"\nğŸ“Š {model_name} æ¨¡å‹ç»“æœ:")
        logging.info(f"  å¹³å‡MAE:  {metrics['mae_avg']:.6f}")
        logging.info(f"  å¹³å‡RMSE: {metrics['rmse_avg']:.6f}")
        logging.info(f"  å¹³å‡RÂ²:   {metrics['r2_score_avg']:.6f}")
        logging.info(f"  å¹³å‡Pearson R: {metrics['pearson_r_avg']:.6f}")
        
        if 'mae_by_horizon' in metrics:
            logging.info("  å„é¢„æµ‹æ—¶æ­¥è¯¦ç»†æŒ‡æ ‡:")
            for h in range(len(metrics['mae_by_horizon'])):
                logging.info(f"    æ—¶æ­¥{h+1:2d}: MAE={metrics['mae_by_horizon'][h]:.6f}, "
                           f"RMSE={metrics['rmse_by_horizon'][h]:.6f}, "
                           f"RÂ²={metrics['r2_by_horizon'][h]:.6f}, "
                           f"Pearson R={metrics['pearson_by_horizon'][h]:.6f}")
    
    # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
    if 'TEC-MoLLM' in results and 'HistoricalAverage' in results:
        logging.info("\nğŸ¯ TEC-MoLLMç›¸å¯¹äºå†å²å¹³å‡çš„æ”¹è¿›:")
        tec_metrics = results['TEC-MoLLM']
        ha_metrics = results['HistoricalAverage']
        
        mae_improvement = ((ha_metrics['mae_avg'] - tec_metrics['mae_avg']) / ha_metrics['mae_avg']) * 100
        rmse_improvement = ((ha_metrics['rmse_avg'] - tec_metrics['rmse_avg']) / ha_metrics['rmse_avg']) * 100
        r2_improvement = ((tec_metrics['r2_score_avg'] - ha_metrics['r2_score_avg']) / abs(ha_metrics['r2_score_avg'])) * 100
        pearson_improvement = ((tec_metrics['pearson_r_avg'] - ha_metrics['pearson_r_avg']) / ha_metrics['pearson_r_avg']) * 100
        
        logging.info(f"  MAEæ”¹è¿›:     {mae_improvement:+.2f}%")
        logging.info(f"  RMSEæ”¹è¿›:    {rmse_improvement:+.2f}%") 
        logging.info(f"  RÂ²æ”¹è¿›:      {r2_improvement:+.2f}%")
        logging.info(f"  Pearson Ræ”¹è¿›: {pearson_improvement:+.2f}%")
    
    logging.info("="*80)
    
    # ä¿å­˜ç»“æœ
    output_path = os.path.join(args.output_dir, "evaluation_results.csv")
    results_df.to_csv(output_path)
    logging.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    # ä¿å­˜ç®€è¦ç»“æœæ‘˜è¦
    summary_path = os.path.join(args.output_dir, "evaluation_summary.txt")
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("TEC-MoLLMæ¨¡å‹è¯„ä¼°ç»“æœæ‘˜è¦\n")
        f.write("="*50 + "\n\n")
        
        for model_name, metrics in results.items():
            f.write(f"{model_name}:\n")
            f.write(f"  å¹³å‡MAE:  {metrics['mae_avg']:.6f}\n")
            f.write(f"  å¹³å‡RMSE: {metrics['rmse_avg']:.6f}\n")
            f.write(f"  å¹³å‡RÂ²:   {metrics['r2_score_avg']:.6f}\n")
            f.write(f"  å¹³å‡Pearson R: {metrics['pearson_r_avg']:.6f}\n\n")
    
    logging.info(f"ç»“æœæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")

if __name__ == '__main__':
    main() 